# DataForSEO Keyword Research Pipeline

Below is a **Jupyter-ready, end-to-end Python flow** you can drop into a notebook to programmatically:

1. **discover seed keywords from scratch**,
2. **expand & qualify them with volumes/CPC/competition**,
3. **discover competitors & scrape their ranked keywords**,
4. **tag intent, cluster into ad buckets, score & shortlist**,
5. **export ready-to-launch ad groups + negatives**.

I've annotated each "Cell" so you can copy/paste straight into Jupyter.

---

## 0) High-level pipeline

```
Config  ➜  Helpers (auth, request, flatten) 
       ➜  Get location/language codes
       ➜  Generate seed keywords (ideas/suggestions + keywords_for_site)
       ➜  Expand & score (vol/CPC/competition/trends)
       ➜  Discover competitors (serp_competitors)
       ➜  Pull competitors' ranked keywords
       ➜  Merge, dedupe, filter (min vol, max CPC, exclude info-intent)
       ➜  Tag search intent (search_intent endpoint)
       ➜  Cluster into buckets (category + TF-IDF/KMeans)
       ➜  Score & prioritize for ads (score formula)
       ➜  Export: ad_groups.csv, negatives.csv, master_keywords.parquet
```

---

## 1) Prereqs

**Python libs**: `requests`, `pandas`, `numpy`, `scikit-learn` (for clustering), `tqdm`, `pyyaml` (optional for configs).

---

## 2) Notebook skeleton

### **Cell 1 — Install deps (if needed)**

```python
%pip install requests pandas numpy scikit-learn tqdm pyyaml
```

### **Cell 2 — Config**

Use a single place to tweak thresholds without touching code.

```python
CONFIG = {
    "dataforseo": {
        "login": "<YOUR_LOGIN>",
        "password": "<YOUR_PASSWORD>",
        "base": "https://api.dataforseo.com/v3"
    },
    "target": {
        "country": "Canada",
        "language": "English",
        "provinces": ["Ontario, Canada", "British Columbia, Canada", "Alberta, Canada"],  # optional, can be empty
    },
    "seed": {
        "business_terms": [
            # start truly from scratch: high-level problems & services
            "private mortgage",
            "bad credit mortgage",
            "bridge financing",
            "second mortgage",
            "home equity loan",
            "fast mortgage approval"
        ],
        "competitor_sites_hint": [
            # fill once you know a few; can leave empty initially
        ]
    },
    "filters": {
        "min_search_volume": 20,
        "max_cpc_cad": 6.0,
        "exclude_if_contains": ["jobs", "salary", "definition", "what is", "meaning"],
        "allowed_intents": ["commercial", "transactional"]  # after intent tagging
    },
    "clustering": {
        "method": "kmeans",  # or "category"
        "k": 12,  # number of clusters if kmeans
        "min_df": 2  # TF-IDF min doc freq
    },
    "export": {
        "dir": "exports",
        "top_n_per_bucket": 100
    }
}
```

### **Cell 3 — Imports & helpers**

```python
import base64, json, time, math
import requests
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from pathlib import Path
from typing import List, Dict

BASE = CONFIG["dataforseo"]["base"]
AUTH_HEADER = {
    "Authorization": "Basic " + base64.b64encode(
        f'{CONFIG["dataforseo"]["login"]}:{CONFIG["dataforseo"]["password"]}'.encode()
    ).decode(),
    "Content-Type": "application/json"
}

def post_dfslabs(endpoint: str, payload: List[dict]) -> dict:
    """Generic Labs API live call."""
    url = f"{BASE}/dataforseo_labs/google/{endpoint}/live"
    r = requests.post(url, headers=AUTH_HEADER, data=json.dumps(payload))
    r.raise_for_status()
    data = r.json()
    # DataForSEO success code is 20000
    if data.get("status_code") != 20000:
        raise RuntimeError(f"DFS API error: {data.get('status_message')}")
    return data

def get_location_code(location_name: str, language_code="en") -> int:
    """Resolve a location_name to DataForSEO location_code."""
    url = f"{BASE}/dataforseo_labs/locations_and_languages"
    r = requests.get(url, headers=AUTH_HEADER)
    r.raise_for_status()
    res = r.json()
    if res.get("status_code") != 20000:
        raise RuntimeError("Could not fetch locations")
    for task in res["tasks"]:
        for loc in task.get("result", []):
            if loc.get("location_name") == location_name and loc.get("language_code") == language_code:
                return loc["location_code"]
    raise ValueError(f"Location not found: {location_name}")

def flatten_task_result(dfslabs_json: dict) -> pd.DataFrame:
    rows = []
    for task in dfslabs_json.get("tasks", []):
        for res in task.get("result", []):
            # Keyword rows can be nested under 'items' or directly under result
            items = res.get("items") or []
            for it in items:
                rows.append(it)
    return pd.DataFrame(rows)
```

### **Cell 4 — Resolve location & language codes**

```python
country_code = get_location_code(CONFIG["target"]["country"])
print("Country code:", country_code)

province_codes = []
for prov in CONFIG["target"]["provinces"]:
    try:
        province_codes.append((prov, get_location_code(prov)))
    except Exception as e:
        print("Could not resolve", prov, e)

language_name = CONFIG["target"]["language"]
```

---

## 3) Seed generation

We'll use three complementary sources:

1. **Keyword Ideas** — category-level ideas around your seed list
2. **Keyword Suggestions** — full-text suggestions that *contain* the seed phrase
3. **Keywords For Site** — if/when you have your (or competitors') domain(s)

### **Cell 5 — Keyword Ideas & Suggestions (Canada-wide)**

```python
def keyword_ideas(seeds: List[str], location_code: int, language_name: str, limit=500):
    payload = [{
        "keywords": seeds[:200],  # max 200 per call
        "location_code": location_code,
        "language_name": language_name,
        "limit": limit
    }]
    return flatten_task_result(post_dfslabs("keyword_ideas", payload))

def keyword_suggestions(seed: str, location_code: int, language_name: str, limit=500):
    payload = [{
        "keyword": seed,
        "location_code": location_code,
        "language_name": language_name,
        "limit": limit
    }]
    return flatten_task_result(post_dfslabs("keyword_suggestions", payload))

all_kw = []

# Keyword Ideas for all seeds in bulk
ideas_df = keyword_ideas(CONFIG["seed"]["business_terms"], country_code, language_name, limit=1000)
all_kw.append(ideas_df)

# Suggestions per seed (gives different flavor of terms)
for kw in tqdm(CONFIG["seed"]["business_terms"]):
    sug_df = keyword_suggestions(kw, country_code, language_name, limit=1000)
    all_kw.append(sug_df)

seed_keywords_df = pd.concat(all_kw, ignore_index=True).drop_duplicates(subset=["keyword"])
print(seed_keywords_df.shape)
seed_keywords_df.head()
```

*(Columns you'll typically see: `keyword`, `cpc`, `competition`, `search_volume`, `search_volume_trend` (list of 12), `categories`, `avg_clicks`, `avg_position`, etc.)*

### **Cell 6 — (Optional) Keywords for Site**

If you have your site or find competitors later, run:

```python
def keywords_for_site(domain: str, location_code: int, language_name: str, limit=1000):
    payload = [{
        "target": domain,
        "location_code": location_code,
        "language_name": language_name,
        "limit": limit
    }]
    return flatten_task_result(post_dfslabs("keywords_for_site", payload))

site_list = CONFIG["seed"]["competitor_sites_hint"]  # empty now; fill later
site_kw_frames = []
for site in site_list:
    site_kw_frames.append(keywords_for_site(site, country_code, language_name, 2000))

if site_kw_frames:
    site_kw_df = pd.concat(site_kw_frames, ignore_index=True)
    seed_keywords_df = (pd.concat([seed_keywords_df, site_kw_df], ignore_index=True)
                          .drop_duplicates(subset=["keyword"]))
```

---

## 4) Discover competitors & pull their keywords

### **Cell 7 — SERP competitors (based on your seeds)**

You need up to 200 keywords to feed in. Use highest volume seeds you already have.

```python
def serp_competitors(keywords: List[str], location_code: int, language_name: str):
    payload = [{
        "keywords": keywords[:200],
        "location_code": location_code,
        "language_name": language_name
    }]
    data = post_dfslabs("serp_competitors", payload)
    rows = []
    for task in data["tasks"]:
        for res in task.get("result", []):
            for item in res.get("items", []):
                rows.append(item)  # each item = a competitor domain with metrics
    return pd.DataFrame(rows)

top_seed_keywords = (seed_keywords_df.sort_values("search_volume", ascending=False)
                     .keyword.head(200).tolist())

competitors_df = serp_competitors(top_seed_keywords, country_code, language_name)
competitors_df.head()
```

Pick the **top N competitor domains** by visibility/etv (estimated traffic).

```python
comp_domains = (competitors_df.sort_values("etv", ascending=False)
                .domain.unique()[:10].tolist())
comp_domains
```

### **Cell 8 — Ranked keywords for each competitor**

```python
def ranked_keywords(domain: str, location_code: int, language_name: str, limit=10000):
    payload = [{
        "target": domain,
        "location_code": location_code,
        "language_name": language_name,
        "limit": limit
    }]
    return flatten_task_result(post_dfslabs("ranked_keywords", payload))

comp_kw_frames = []
for d in tqdm(comp_domains):
    try:
        df = ranked_keywords(d, country_code, language_name, 10000)
        df["source_domain"] = d
        comp_kw_frames.append(df)
    except Exception as e:
        print("fail", d, e)

competitor_kw_df = pd.concat(comp_kw_frames, ignore_index=True) if comp_kw_frames else pd.DataFrame()
competitor_kw_df.head()
```

---

## 5) Merge, dedupe, basic filtering

### **Cell 9 — Merge everything & initial clean**

```python
master_df = (pd.concat([seed_keywords_df, competitor_kw_df], ignore_index=True, sort=False)
               .drop_duplicates(subset=["keyword"])
               .reset_index(drop=True))

def contains_any(text, phrases):
    t = text.lower()
    return any(p in t for p in phrases)

f = CONFIG["filters"]
mask = (
    (master_df["search_volume"].fillna(0) >= f["min_search_volume"]) &
    (master_df["cpc"].fillna(0) <= f["max_cpc_cad"]) &
    (~master_df["keyword"].astype(str).apply(lambda x: contains_any(x, f["exclude_if_contains"])))
)
filtered_df = master_df[mask].copy()
print(master_df.shape, "->", filtered_df.shape)
filtered_df.head()
```

---

## 6) Intent tagging (commercial / transactional only)

### **Cell 10 — Call Search Intent endpoint**

DataForSEO Labs: `/search_intent/live` (up to 1000 keywords per call). We'll batch.

```python
def search_intent(keywords: List[str], location_code: int, language_name: str):
    payload = [{
        "keywords": keywords,
        "location_code": location_code,
        "language_name": language_name
    }]
    return flatten_task_result(post_dfslabs("search_intent", payload))

def batch(iterable, n=1000):
    l = len(iterable)
    for ndx in range(0, l, n):
        yield iterable[ndx:ndx+n]

intent_frames = []
for chunk in tqdm(list(batch(filtered_df["keyword"].tolist(), 1000))):
    try:
        df = search_intent(chunk, country_code, language_name)
        intent_frames.append(df)
    except Exception as e:
        print("intent batch failed", e)

if intent_frames:
    intent_df = pd.concat(intent_frames, ignore_index=True)
    # join on keyword
    filtered_df = filtered_df.merge(intent_df[["keyword", "intent"]], on="keyword", how="left")
else:
    filtered_df["intent"] = np.nan

allowed_intents = CONFIG["filters"]["allowed_intents"]
post_intent_df = filtered_df[filtered_df["intent"].isin(allowed_intents)]
print(filtered_df.shape, "->", post_intent_df.shape)
```

---

## 7) Clustering into ad buckets

Two easy modes:

1. **Category buckets** (cheap & reliable if `categories` exist)
2. **TF-IDF + KMeans** (for text-based clustering when categories missing/noisy)

### **Cell 11A — Category-based buckets (fast path)**

```python
def extract_primary_category(cats):
    if isinstance(cats, list) and cats:
        return cats[0]
    return "uncategorized"

if "categories" in post_intent_df.columns:
    post_intent_df["bucket"] = post_intent_df["categories"].apply(extract_primary_category)
else:
    post_intent_df["bucket"] = "uncategorized"

post_intent_df["bucket"].value_counts().head(20)
```

### **Cell 11B — KMeans text clustering (if you prefer ML buckets)**

```python
if CONFIG["clustering"]["method"] == "kmeans":
    vec = TfidfVectorizer(ngram_range=(1,2), min_df=CONFIG["clustering"]["min_df"])
    X = vec.fit_transform(post_intent_df["keyword"])
    k = CONFIG["clustering"]["k"]
    km = KMeans(n_clusters=k, random_state=42, n_init="auto")
    clusters = km.fit_predict(X)
    post_intent_df["bucket"] = clusters.astype(str)
```

---

## 8) Scoring & prioritization

Create a simple **keyword score** to sort inside each bucket:

Example:
`score = norm(search_volume) * norm(1 / (cpc+1)) * norm(avg_clicks or ETV if present)`

### **Cell 12 — Score**

```python
def norm(s):
    s = s.fillna(0)
    if s.max() == s.min():
        return pd.Series([1]*len(s), index=s.index)
    return (s - s.min()) / (s.max() - s.min())

post_intent_df["nv"] = norm(post_intent_df["search_volume"])
post_intent_df["ncpc"] = norm(1 / (post_intent_df["cpc"].fillna(0) + 0.01))
post_intent_df["netv"] = norm(post_intent_df.get("etv", pd.Series(np.nan, index=post_intent_df.index)))
post_intent_df["score"] = post_intent_df[["nv", "ncpc", "netv"]].fillna(0).mean(axis=1)

ranked_df = (post_intent_df
             .sort_values(["bucket", "score"], ascending=[True, False])
             .reset_index(drop=True))
ranked_df.head()
```

---

## 9) Export deliverables for Ads & analysis

### **Cell 13 — Build ad-group ready outputs**

* **ad\_groups.csv**: one row/keyword with bucket = ad group name
* **negatives.csv**: everything filtered out / low intent
* **master\_keywords.parquet**: full dataset with all metrics

```python
Path(CONFIG["export"]["dir"]).mkdir(exist_ok=True, parents=True)

# Top-N per bucket for tight ad groups
topN = CONFIG["export"]["top_n_per_bucket"]
ad_groups = (ranked_df.groupby("bucket")
             .head(topN)
             .sort_values(["bucket", "score"], ascending=[True, False]))
ad_groups.to_csv(f'{CONFIG["export"]["dir"]}/ad_groups.csv', index=False)

# negatives = keywords you filtered out earlier + anything below thresholds
negatives = master_df[~master_df["keyword"].isin(ranked_df["keyword"])]
negatives.to_csv(f'{CONFIG["export"]["dir"]}/negatives.csv', index=False)

# full data
ranked_df.to_parquet(f'{CONFIG["export"]["dir"]}/master_keywords.parquet')

print("Exported: ad_groups.csv, negatives.csv, master_keywords.parquet")
```

---

## 10) (Optional) Province-level refresh

Repeat **Keyword Suggestions / Ideas** for each `province_code` (from Cell 4), then **add a `geo` column** and compare volumes to spot regional winners. You can re-use the same helper functions, just swap `location_code`.

---

## 11) (Optional) Daily/weekly refresh

Create a tiny scheduler (cron/GitHub Actions/Airflow) to re-run:

1. Suggestions/Ideas (to catch new long tails)
2. SERP Competitors (to watch new entrants)
3. Ranked Keywords for top 5 competitors
4. Re-tag intent & re-score
5. Diff vs. yesterday: alert when new **high-score** keywords appear

---

## 12) Guardrails & costs

* **Rate limits**: Labs API live supports **\~2000 calls/min**; you're safe with batching.
* **Costs**: Labs API is pay-as-you-go (roughly `$0.01/task + $0.0001/item`); use filters & limits to avoid over-pulling.
* **Data hygiene**: Always **dedupe by `keyword`**, and **log which endpoint produced the keyword** (seed vs competitor) for attribution.

---

### You now have:

* A runnable **Jupyter pipeline** to go from zero → **prioritized, intent-qualified, clustered keyword sets**.
* **Exports** you can hand straight to your Google Ads build (buckets → ad groups, negatives list, CPC/Volume for bid caps).
* A foundation you can schedule to auto-refresh weekly.

If you want, send me your **first run's CSVs**, and I'll help you tighten the filters, bucket names, and scoring formula for **maximum conversion yield**.