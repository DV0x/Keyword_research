{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Ads Keyword Parser\n",
    "**Agency-ready workflow to turn ranked_keywords CSV into bidding-ready data**\n",
    "\n",
    "Based on the simple, repeatable recipe for processing huge competitor keyword exports.\n",
    "\n",
    "You only edit two things:\n",
    "1. The **RAW** filename in Step 1\n",
    "2. The freshness windows in Step 5 (default: metrics ≤ 90 days, ranks ≤ 30 days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 — Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this once if packages aren't installed\n",
    "# !pip install pandas numpy python-dateutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 — Load the file & pick the useful columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, ast, json, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "RAW = Path(\"data/competitor_keywords_v2.csv\")   # <-- your file\n",
    "df = pd.read_csv(RAW, engine=\"python\")         # engine=python tolerates commas in JSON\n",
    "\n",
    "print(f\"📊 Loaded {len(df):,} rows from {RAW}\")\n",
    "\n",
    "# --- choose ONLY the columns you care about ---\n",
    "cols = [\n",
    "    \"keyword\",                              # query\n",
    "    \"keyword_search_volume\",                # last-month volume\n",
    "    \"keyword_cpc\",                          # avg CPC\n",
    "    \"keyword_competition\",                  # 0-100 index\n",
    "    \"keyword_low_top_of_page_bid\",          # low-range bid\n",
    "    \"keyword_high_top_of_page_bid\",         # high-range bid\n",
    "    \"keyword_difficulty\",                   # SEO KD\n",
    "    \"keyword_search_intent_info\",           # intent JSON\n",
    "    \"keyword_last_updated_time\",            # when volume/CPC were refreshed\n",
    "    \"ranked_serp_element\"                   # blob that holds competitor rank + timestamp\n",
    "]\n",
    "df = df[cols]\n",
    "print(f\"✅ Selected {len(cols)} essential columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 — Flatten the two JSON blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Intent ---\n",
    "df[\"intent\"] = df[\"keyword_search_intent_info\"].apply(\n",
    "    lambda s: ast.literal_eval(s)[\"main_intent\"] if pd.notna(s) else \"unknown\"\n",
    ")\n",
    "\n",
    "# --- Competitor rank & rank timestamp ---\n",
    "serp = df[\"ranked_serp_element\"].apply(ast.literal_eval)\n",
    "df[\"competitor_rank\"] = serp.map(lambda j: j[\"serp_item\"][\"rank_absolute\"])\n",
    "df[\"rank_date\"] = pd.to_datetime(\n",
    "    serp.map(lambda j: j[\"last_updated_time\"]), errors=\"coerce\"\n",
    ")\n",
    "\n",
    "print(f\"🔍 Extracted intent and competitor ranking data\")\n",
    "print(f\"Intent distribution: {df['intent'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to keep the original JSON columns once you've extracted what you need\n",
    "df.drop(columns=[\"keyword_search_intent_info\",\"ranked_serp_element\"], inplace=True)\n",
    "print(f\"🗑️  Dropped JSON columns, now have {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 — Light cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric cast\n",
    "num_cols = [\"keyword_search_volume\",\"keyword_cpc\",\"keyword_competition\",\n",
    "            \"keyword_low_top_of_page_bid\",\"keyword_high_top_of_page_bid\",\"keyword_difficulty\"]\n",
    "df[num_cols] = df[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# fix volumes if they look tiny (older exports store 8.3 instead of 830)\n",
    "if df[\"keyword_search_volume\"].max() < 50:\n",
    "    df[\"keyword_search_volume\"] = (df[\"keyword_search_volume\"] * 100).round(0)\n",
    "    print(\"📈 Fixed volume scaling (multiplied by 100)\")\n",
    "else:\n",
    "    print(f\"📊 Volume range looks good: {df['keyword_search_volume'].min():.0f} - {df['keyword_search_volume'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 — Drop obvious junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junk = [\"calculator\",\"meaning\",\"definition\",\"job\",\"salary\"]\n",
    "before_count = len(df)\n",
    "mask = ~df[\"keyword\"].str.contains(\"|\".join(junk), case=False, na=False)\n",
    "df = df[mask]\n",
    "removed = before_count - len(df)\n",
    "print(f\"🗑️  Removed {removed:,} junk keywords ({removed/before_count*100:.1f}%)\")\n",
    "print(f\"✅ Clean dataset: {len(df):,} keywords remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 — Keep only *fresh* rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "now = datetime.utcnow()\n",
    "\n",
    "df[\"kw_date\"] = pd.to_datetime(df[\"keyword_last_updated_time\"], errors=\"coerce\")\n",
    "\n",
    "before_fresh = len(df)\n",
    "df = df[\n",
    "    (df.kw_date >= now - timedelta(days=90)) &     # metrics ≤ 3 months old\n",
    "    (df.rank_date >= now - timedelta(days=30))     # ranking snapshot ≤ 1 month old\n",
    "]\n",
    "removed_stale = before_fresh - len(df)\n",
    "print(f\"🕒 Removed {removed_stale:,} stale rows (metrics >90 days or ranks >30 days)\")\n",
    "print(f\"✅ Fresh dataset: {len(df):,} keywords with recent data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 — Rename to friendly headers & reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    \"keyword_search_volume\":\"vol\",\n",
    "    \"keyword_cpc\":\"cpc\",\n",
    "    \"keyword_competition\":\"competition\",\n",
    "    \"keyword_low_top_of_page_bid\":\"low_bid\",\n",
    "    \"keyword_high_top_of_page_bid\":\"high_bid\",\n",
    "    \"keyword_difficulty\":\"kd\"\n",
    "})[[\n",
    "    \"keyword\",\"vol\",\"cpc\",\"competition\",\"low_bid\",\"high_bid\",\n",
    "    \"kd\",\"intent\",\"competitor_rank\",\"kw_date\",\"rank_date\"\n",
    "]]\n",
    "\n",
    "print(\"✅ Renamed columns for Google Ads readiness\")\n",
    "print(f\"Final columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 — Export for bidding / sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "OUT = Path(f\"bidding_ready_{timestamp}.csv\")\n",
    "\n",
    "# Sort by volume (desc) and competitor rank (asc) for easy bidding prioritization\n",
    "df_sorted = df.sort_values([\"vol\",\"competitor_rank\"], ascending=[False,True])\n",
    "df_sorted.to_csv(OUT, index=False)\n",
    "\n",
    "print(f\"✅ Saved {OUT.resolve()} with {len(df):,} rows\")\n",
    "print(f\"🎯 Ready for Google Ads import!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary stats\n",
    "print(\"📊 FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total keywords: {len(df):,}\")\n",
    "print(f\"Volume range: {df['vol'].min():.0f} - {df['vol'].max():.0f}\")\n",
    "print(f\"CPC range: ${df['cpc'].min():.2f} - ${df['cpc'].max():.2f}\")\n",
    "print(f\"Intent breakdown: {df['intent'].value_counts().to_dict()}\")\n",
    "print(\"\\n🎯 TOP 10 HIGHEST VOLUME KEYWORDS:\")\n",
    "display(df_sorted[['keyword', 'vol', 'cpc', 'intent', 'competitor_rank']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read the sheet tomorrow morning\n",
    "\n",
    "| Column | Why you care |\n",
    "|--------|-------------|\n",
    "| **keyword** | Copy into Ads Editor. |\n",
    "| **vol** | Anything 20+ searches/month deserves its own ad group; consolidate smaller ones. |\n",
    "| **cpc** | Multiply × 1.2 for your initial Max CPC bid. |\n",
    "| **competition** | 0-100; > 70 means a crowded auction—double-check lander quality & budget. |\n",
    "| **low_bid / high_bid** | Google's bid range for top-of-page. Stay near the low end while you test. |\n",
    "| **kd** | < 40 → publish an SEO page later; 40-60 = build links; > 60 ignore for SEO. |\n",
    "| **intent** | Keep **commercial / transactional** on day 1; everything else can wait or become a negative. |\n",
    "| **competitor_rank** | If the rival is already #1 organically, your ad may need extra punch (call-out, 24h approval, etc.). |\n",
    "| **kw_date / rank_date** | Sanity check freshness—if something slipped through older than the window, refresh in DataForSEO first. |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}